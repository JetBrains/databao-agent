"""Suggested questions generation for the Databao chat interface."""

import logging
import threading
from concurrent.futures import Future, ThreadPoolExecutor
from concurrent.futures import TimeoutError as FuturesTimeoutError
from typing import TYPE_CHECKING

import streamlit as st
from langchain_core.messages import HumanMessage, SystemMessage
from pydantic import BaseModel, Field

if TYPE_CHECKING:
    from databao.core.agent import Agent

logger = logging.getLogger(__name__)

# Type alias for suggestion results
SuggestionsResult = tuple[list[str], bool] | None


@st.cache_resource
def _get_suggestions_executor() -> ThreadPoolExecutor:
    """Get a shared thread pool executor for suggestions generation."""
    return ThreadPoolExecutor(max_workers=1, thread_name_prefix="suggestions")


# Fallback questions when LLM generation fails
FALLBACK_QUESTIONS = [
    "What tables are available in my database?",
    "Show me a summary of the main data",
    "What are the key metrics I can analyze?",
]


class SuggestedQuestions(BaseModel):
    """Structured output model for suggested questions."""

    questions: list[str] = Field(
        description="List of exactly 3 short questions about the data",
        min_length=3,
        max_length=3,
    )


def _build_context_from_sources(agent: "Agent") -> str:
    """Extract schema/context information from agent sources."""
    context_parts: list[str] = []

    # Add database contexts
    for name, db_source in agent.dbs.items():
        if db_source.context:
            context_parts.append(f"Database '{name}':\n{db_source.context}")

    # Add dataframe contexts
    for name, df_source in agent.dfs.items():
        if df_source.context:
            context_parts.append(f"DataFrame '{name}':\n{df_source.context}")

    # Add additional context
    for ctx in agent.additional_context:
        context_parts.append(ctx)

    if not context_parts:
        return "No detailed schema information available."

    return "\n\n".join(context_parts)


SUGGESTION_PROMPT = """You are helping suggest questions for a data analysis assistant.
The user has connected the following data sources:

{context}

Generate exactly 3 short questions (maximum 60 characters each) that:
1. Would produce interesting data tables or visualizations
2. Are answerable using the available data
3. Cover different aspects of the data (e.g., summaries, trends, comparisons)

The questions should be simple and direct, suitable for display as clickable buttons.
Do not include numbering or bullet points in the questions themselves."""


def generate_suggested_questions(agent: "Agent") -> tuple[list[str], bool]:
    """Generate suggested questions based on the agent's data sources.

    Uses the agent's LLM with structured output to generate contextual questions.
    Falls back to generic questions if LLM generation fails.

    Args:
        agent: The Databao agent with configured data sources.

    Returns:
        A tuple of (questions, is_llm_generated) where:
        - questions: List of 3 question strings
        - is_llm_generated: True if questions were generated by LLM, False if fallback
    """
    try:
        # Build context from agent sources
        context = _build_context_from_sources(agent)

        # Create the prompt
        prompt = SUGGESTION_PROMPT.format(context=context)

        # Use structured output for reliable extraction
        llm_with_structure = agent.llm.with_structured_output(SuggestedQuestions)

        # Generate questions
        result = llm_with_structure.invoke([
            SystemMessage(content=prompt),
            HumanMessage(content="Generate 3 suggested questions for exploring this data."),
        ])

        if result and result.questions and len(result.questions) == 3:
            # Return questions as-is - UI will handle truncation for display
            logger.info("Successfully generated %d suggested questions via LLM", len(result.questions))
            return list(result.questions), True

        logger.warning("LLM returned invalid result, using fallback questions")
        return FALLBACK_QUESTIONS.copy(), False

    except Exception as e:
        logger.warning("Failed to generate suggested questions: %s. Using fallback.", e)
        return FALLBACK_QUESTIONS.copy(), False


def _generate_suggestions_task(
    agent: "Agent",
    cancel_event: threading.Event,
) -> SuggestionsResult:
    """Background task that generates suggestions with cancellation support.

    Args:
        agent: The Databao agent.
        cancel_event: Event to signal cancellation.

    Returns:
        Tuple of (questions, is_llm_generated) or None if cancelled.
    """
    # Check cancellation before starting expensive LLM call
    if cancel_event.is_set():
        logger.info("Suggestions generation cancelled before starting")
        return None

    try:
        questions, is_llm_generated = generate_suggested_questions(agent)

        # Check again after LLM call (user may have asked during generation)
        if cancel_event.is_set():
            logger.info("Suggestions generation cancelled after LLM call")
            return None

        return questions, is_llm_generated

    except Exception as e:
        logger.warning("Background suggestions generation failed: %s", e)
        if cancel_event.is_set():
            return None
        return FALLBACK_QUESTIONS.copy(), False


def start_suggestions_generation(agent: "Agent") -> bool:
    """Start background suggestions generation.

    Returns True if generation was started, False if already running or ready.
    """
    status = st.session_state.get("suggestions_status", "not_started")

    if status != "not_started":
        return False  # Already loading or ready

    # Create cancellation event
    cancel_event = threading.Event()

    # Submit task to executor
    executor = _get_suggestions_executor()
    future: Future[SuggestionsResult] = executor.submit(
        _generate_suggestions_task, agent, cancel_event
    )

    # Store in session state
    st.session_state.suggestions_future = future
    st.session_state.suggestions_cancel_event = cancel_event
    st.session_state.suggestions_status = "loading"

    logger.info("Started background suggestions generation")
    return True


def cancel_suggestions_generation() -> None:
    """Cancel any pending suggestions generation.

    Call this when the user submits their own question.
    """
    cancel_event: threading.Event | None = st.session_state.get("suggestions_cancel_event")
    if cancel_event is not None:
        cancel_event.set()
        logger.info("Cancelled suggestions generation")

    # Clean up state
    st.session_state.suggestions_future = None
    st.session_state.suggestions_cancel_event = None

    # Only mark as cancelled if was loading (don't overwrite 'ready')
    if st.session_state.get("suggestions_status") == "loading":
        st.session_state.suggestions_status = "cancelled"


def reset_suggestions_state() -> None:
    """Reset suggestions state to allow regeneration.

    Call this when starting a new chat or when the user wants fresh suggestions.
    """
    # Cancel any in-progress generation first
    cancel_event: threading.Event | None = st.session_state.get("suggestions_cancel_event")
    if cancel_event is not None:
        cancel_event.set()

    # Reset all suggestions-related state
    st.session_state.suggestions_future = None
    st.session_state.suggestions_cancel_event = None
    st.session_state.suggestions_status = "not_started"
    st.session_state.suggested_questions = []
    st.session_state.suggestions_are_llm_generated = False

    logger.info("Reset suggestions state")


def check_suggestions_completion() -> bool:
    """Check if background suggestions generation has completed.

    If completed, updates session state with results.

    Returns True if suggestions became ready (caller should rerun to show them).
    """
    # Early exit if status is not loading (already completed or cancelled)
    status = st.session_state.get("suggestions_status")
    if status != "loading":
        return False

    future: Future[SuggestionsResult] | None = st.session_state.get("suggestions_future")

    if future is None:
        return False

    if not future.done():
        # Check for timeout - if we've been waiting too long, use fallback
        # Note: We can't easily track start time here without adding more state,
        # so timeout is handled by letting the executor manage it naturally
        return False

    # Get result with a small timeout as safety (should be instant since done=True)
    try:
        result = future.result(timeout=1.0)
    except FuturesTimeoutError:
        logger.warning("Timeout getting completed future result")
        result = None
    except Exception as e:
        logger.warning("Failed to get suggestions result: %s", e)
        result = None

    # Clean up future
    st.session_state.suggestions_future = None
    st.session_state.suggestions_cancel_event = None

    # Handle result
    if result is None:
        # Was cancelled or failed - use fallback instead of showing nothing
        st.session_state.suggested_questions = FALLBACK_QUESTIONS.copy()
        st.session_state.suggestions_are_llm_generated = False
        st.session_state.suggestions_status = "ready"
        logger.info("Suggestions generation failed/cancelled, using fallback")
        return True

    questions, is_llm_generated = result
    st.session_state.suggested_questions = questions
    st.session_state.suggestions_are_llm_generated = is_llm_generated
    st.session_state.suggestions_status = "ready"

    logger.info("Suggestions generation completed with %d questions", len(questions))
    return True


def is_suggestions_loading() -> bool:
    """Check if suggestions are currently being generated.

    Useful for the polling fragment to know when to stop.
    """
    return st.session_state.get("suggestions_status") == "loading"
